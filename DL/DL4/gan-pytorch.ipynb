{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. GAN first introduction\n![GAN picture](./gan.png)\n\n\n\nGANs are a class of unsupervised generative models which implicitly model the data density.\n\nThe basic setup is pictured above. There are two \"competing\" neural networks:\n* The Generator wants to learn to generate realistic images that are indistinguishable from the real data. \n    - *input*: Gaussian noise random sample. *output*: a (higher dimensional) datapoint\n* The Discriminator wants to tell the real & fake images apart.\n    - *input*: datapoint/image, *output*: probability assigned to datapoint being real. Think binary classifier.\n* The typical analogy: the generator is like a counterfeiter trying to look like real, the discriminator is the police trying to tell counterfeits from the real work.\n* The key novelty of GANs is to pass the error signal (gradients) from the discriminator to the generator: the generator neural network uses the information from the competing discriminator neural network to know how to produce more realistic output.","metadata":{}},{"cell_type":"markdown","source":"### 2. Define the neural networks in pytorch","metadata":{}},{"cell_type":"code","source":"import sys\nprint(sys.version) # python 3.6\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torchvision.utils as vutils\nprint(torch.__version__) # 1.0.1\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndef show_imgs(x, new_fig=True):\n    grid = vutils.make_grid(x.detach().cpu(), nrow=8, normalize=True, pad_value=0.3)\n    grid = grid.transpose(0,2).transpose(0,1) # channels as last dimension\n    if new_fig:\n        plt.figure()\n    plt.imshow(grid.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:05:53.476904Z","iopub.execute_input":"2022-06-17T15:05:53.477308Z","iopub.status.idle":"2022-06-17T15:05:55.929160Z","shell.execute_reply.started":"2022-06-17T15:05:53.477234Z","shell.execute_reply":"2022-06-17T15:05:55.928355Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Let's define a small 2-layer fully connected neural network (so one hidden layer) for the discriminator D:","metadata":{}},{"cell_type":"code","source":"class Discriminator(torch.nn.Module):\n    def __init__(self, inp_dim=784):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(inp_dim, 128)\n        self.nonlin1 = nn.LeakyReLU(0.2)\n        self.fc2 = nn.Linear(128, 1)\n    def forward(self, x):\n        x = x.view(x.size(0), 784) # flatten (bs x 1 x 28 x 28) -> (bs x 784)\n        h = self.nonlin1(self.fc1(x))\n        out = self.fc2(h)\n        out = torch.sigmoid(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:05:55.933299Z","iopub.execute_input":"2022-06-17T15:05:55.935845Z","iopub.status.idle":"2022-06-17T15:05:55.945149Z","shell.execute_reply.started":"2022-06-17T15:05:55.935799Z","shell.execute_reply":"2022-06-17T15:05:55.944410Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"And a small 2-layer neural network for the generator G. G takes a 100-dimensional noise vector and generates an output of the size matching the data.","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim=100):\n        super(Generator, self).__init__()\n        self.fc1 = nn.Linear(z_dim, 128)\n        self.nonlin1 = nn.LeakyReLU(0.2)\n        self.fc2 = nn.Linear(128, 784)\n    def forward(self, x):\n        h = self.nonlin1(self.fc1(x))\n        out = self.fc2(h)\n        out = torch.tanh(out) # range [-1, 1]\n        # convert to image \n        out = out.view(out.size(0), 1, 28, 28)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:05:55.949668Z","iopub.execute_input":"2022-06-17T15:05:55.952457Z","iopub.status.idle":"2022-06-17T15:05:55.960544Z","shell.execute_reply.started":"2022-06-17T15:05:55.952421Z","shell.execute_reply":"2022-06-17T15:05:55.959670Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# instantiate a Generator and Discriminator according to their class definition.\nD = Discriminator()\nprint(D)\nG = Generator()\nprint(G)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:05:55.963006Z","iopub.execute_input":"2022-06-17T15:05:55.963670Z","iopub.status.idle":"2022-06-17T15:05:55.980021Z","shell.execute_reply.started":"2022-06-17T15:05:55.963630Z","shell.execute_reply":"2022-06-17T15:05:55.979279Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Loading the data and computing forward pass","metadata":{}},{"cell_type":"code","source":"# let's download the Fashion MNIST data, if you do this locally and you downloaded before,\n# you can change data paths to point to your existing files\n# dataset = torchvision.datasets.MNIST(root='./MNISTdata', ...)\ndataset = torchvision.datasets.FashionMNIST(root='./FashionMNIST/',\n                       transform=transforms.Compose([transforms.ToTensor(),\n                                                     transforms.Normalize((0.5,), (0.5,))]),\n                       download=True)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:05:55.983745Z","iopub.execute_input":"2022-06-17T15:05:55.984208Z","iopub.status.idle":"2022-06-17T15:06:16.363030Z","shell.execute_reply.started":"2022-06-17T15:05:55.984183Z","shell.execute_reply":"2022-06-17T15:06:16.362178Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Let's look at a sample:","metadata":{}},{"cell_type":"code","source":"ix=149\nx, _ = dataset[ix]\nplt.matshow(x.squeeze().numpy(), cmap=plt.cm.gray)\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:16.364405Z","iopub.execute_input":"2022-06-17T15:06:16.364781Z","iopub.status.idle":"2022-06-17T15:06:16.749285Z","shell.execute_reply.started":"2022-06-17T15:06:16.364730Z","shell.execute_reply":"2022-06-17T15:06:16.747620Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# for one image:\nDscore = D(x)\nDscore","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:16.750690Z","iopub.execute_input":"2022-06-17T15:06:16.751086Z","iopub.status.idle":"2022-06-17T15:06:16.835311Z","shell.execute_reply.started":"2022-06-17T15:06:16.751044Z","shell.execute_reply":"2022-06-17T15:06:16.834222Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# How you can get a batch of images from the dataloader:\nxbatch, _ = iter(dataloader).next() # 64 x 1 x 28 x 28: minibatch of 64 samples\nxbatch.shape\nD(xbatch) # 64x1 tensor: 64 predictions of probability of input being real.\nD(xbatch).shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:16.839667Z","iopub.execute_input":"2022-06-17T15:06:16.840207Z","iopub.status.idle":"2022-06-17T15:06:16.889489Z","shell.execute_reply.started":"2022-06-17T15:06:16.840138Z","shell.execute_reply":"2022-06-17T15:06:16.888691Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"show_imgs(xbatch)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:16.893452Z","iopub.execute_input":"2022-06-17T15:06:16.895670Z","iopub.status.idle":"2022-06-17T15:06:17.142622Z","shell.execute_reply.started":"2022-06-17T15:06:16.895632Z","shell.execute_reply":"2022-06-17T15:06:17.141829Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 3 Intermezzo: optimization with SGD - linear regression example\nWe will step away from GANs for a second to introduce the core of deep learning: optimization with SGD.\n\nHere are the core components of a basic deep learning classifier/regression setup:\n* a neural network $\\hat{y}=f(x, \\theta)$, which takes an input $x$ and parameters $\\theta$, and outputs $\\hat{y}$, a prediction of label $y$.\n* a loss function $\\mathcal{L}(\\theta) = \\mathbb{E}_{x,y \\sim p_d} \\ell(f(x, \\theta), y) \\approx \\sum_{x_i,y_i \\sim mb} \\ell(f(x_i, \\theta), y_i)$.\n* optimizing $\\theta$ to reduce the loss, by making small updates to $\\theta$ in the direction of $-\\nabla_\\theta \\mathcal{L}(\\theta)$.\n\npytorch is designed around these core components:\n* The way to define a neural network is with `torch.nn.Module`, see how we defined the Discriminator and Generator above.\n    - a `Module` defines (1) its weights and (2) defines the operations done with them.\n    - initializing a module initializes the weights at random\n* $\\theta$ stands for all our neural network weights (everything you get from `.parameters()`)\n* In the optimization loop you will evaluate a \"minibatch\" of samples (in our case 64) to compute the neural network output, and the loss measuring how far away those predictions are from the truth.\n* To compute the gradient $\\nabla_\\theta \\mathcal{L}(\\theta)$, you call `.backward()` on the loss. This is where the magic happens: the gradient wrt all weights in the neural network is computed. They appear in a new Tensor `p.grad` for each `p in net.parameters()`\n    - under the hood, this happens by keeping track of the computational graph, and reversing the computation order to \"backpropagate\" the loss with the chain rule.\n    - [Figure which shows a bit more detail](https://tom.sercu.me/assets/201812CCNY/NN_fig.pdf)","metadata":{}},{"cell_type":"markdown","source":"### 3a: basic autograd example\n\nSo we said the big deal about pytorch (or other deep learning package) is **autograd = automatic differentiation** which allows to compute derivatives automatically.\n\nEvery `torch.Tensor`, let's say `x`, has an important flag `requires_grad`. If this flag is set to True, pytorch will keep track of the graph of operations that happen with this tensor.\nWhen we finally arrive at some output (a scalar variable based on a sequence of operations on `x`), we can call `.backward()` on this output, to compute the gradient `d(output) / dx`. This gradient will end up in `x.grad`.","metadata":{}},{"cell_type":"code","source":"x = torch.randn(2,2, requires_grad=True)\nx","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.145745Z","iopub.execute_input":"2022-06-17T15:06:17.146288Z","iopub.status.idle":"2022-06-17T15:06:17.157894Z","shell.execute_reply.started":"2022-06-17T15:06:17.146252Z","shell.execute_reply":"2022-06-17T15:06:17.156827Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# no gradient yet at this point:\nprint(x.grad)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.159712Z","iopub.execute_input":"2022-06-17T15:06:17.161331Z","iopub.status.idle":"2022-06-17T15:06:17.167981Z","shell.execute_reply.started":"2022-06-17T15:06:17.161295Z","shell.execute_reply":"2022-06-17T15:06:17.167029Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"y=(x**2 + x)\nz = y.sum()\nz","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.169406Z","iopub.execute_input":"2022-06-17T15:06:17.171204Z","iopub.status.idle":"2022-06-17T15:06:17.186584Z","shell.execute_reply.started":"2022-06-17T15:06:17.171162Z","shell.execute_reply":"2022-06-17T15:06:17.185733Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We know from high school math that the derivative `dz / dx[i,j]` = 2*x[i,j] +1","metadata":{}},{"cell_type":"code","source":"z.backward()\nx.grad","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.187936Z","iopub.execute_input":"2022-06-17T15:06:17.188911Z","iopub.status.idle":"2022-06-17T15:06:17.257423Z","shell.execute_reply.started":"2022-06-17T15:06:17.188873Z","shell.execute_reply":"2022-06-17T15:06:17.256610Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"2*x+1","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.258683Z","iopub.execute_input":"2022-06-17T15:06:17.259159Z","iopub.status.idle":"2022-06-17T15:06:17.265841Z","shell.execute_reply.started":"2022-06-17T15:06:17.259124Z","shell.execute_reply":"2022-06-17T15:06:17.264749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"You can play with this: you can introduce any tensor operation here; for example `torch.exp(torch.sin(x**2))`. Confirm that the gradient matches the analytical derivative.","metadata":{}},{"cell_type":"markdown","source":"More about autograd in the tutorial https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py and the docs https://pytorch.org/docs/stable/autograd.html\n\n\nThis was a very basic example of what pytorch autograd does for us: computing the derivatives of a scalar function $z(x)$ wrt $x$: $\\nabla_x z(x)$.\nIn a deep learning context this will be at the basis of our optimization; now we will have\n* $\\mathcal{L}(\\theta)$  the loss is a (scalar) function of neural network parameters (vector) $\\theta$.\n* autograd will allow us to call `.backward()` on the loss, which will compute the gradient of the loss with respect to neural network parameters $\\nabla_\\theta \\mathcal{L}(\\theta)$.\n* For each of the parameters `p` the gradient will be in `p.grad`\n* Can you confirm that for the parameters of G/D, the flag `.requires_grad` is `True`?","metadata":{}},{"cell_type":"code","source":"for p in G.parameters():\n    print(p.grad)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.267415Z","iopub.execute_input":"2022-06-17T15:06:17.268051Z","iopub.status.idle":"2022-06-17T15:06:17.273574Z","shell.execute_reply.started":"2022-06-17T15:06:17.268007Z","shell.execute_reply":"2022-06-17T15:06:17.272667Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 3b: Linear regression\nLet's try this for a simple linear mapping `y = f(x, theta) = <x, theta>` with $x, \\theta \\in \\mathbb{R}^{2}$. We we want to optimize $\\theta$:","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(23231)\nx1 = torch.Tensor([1, 2, 3, -3, -2])\ny = torch.Tensor ([3, 6, 9, -9, -6]).view(5,1)\nx2 = torch.randn(5)\nx = torch.stack([x1, x2], dim=1) # 5 x 2 input. 5 datapoints, 2 dimensions.\n# theta = torch.randn(1,2, requires_grad=True) # ~equal to:\ntheta = torch.nn.Parameter(torch.randn(1,2))\n# we start theta at random initialization, the gradient will point us in the right direction.\nprint('x:\\n', x)\nprint('y:\\n', y)\nprint('theta at random initialization: ', theta)\nthetatrace = [theta.data.clone()] # initial value, for logging","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.275059Z","iopub.execute_input":"2022-06-17T15:06:17.275764Z","iopub.status.idle":"2022-06-17T15:06:17.290446Z","shell.execute_reply.started":"2022-06-17T15:06:17.275720Z","shell.execute_reply":"2022-06-17T15:06:17.289126Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Take a look at x and y. What is their correct (linear) relationship?\n\nA: `y = 3 x1 + 0 x2`","metadata":{}},{"cell_type":"markdown","source":"Now we define a prediction as a linear mapping $\\hat{y} = (X . \\theta)$\n\nWe will compute the ordinary least squares objective (mean squared error):  $\\mathcal{L}(\\theta) = (\\hat{y}(x,\\theta) - y)^2$\n\nCompute $\\nabla_\\theta \\mathcal{L}(\\theta)$, and\n\nMove $\\theta$ a small step opposite to that direction","metadata":{}},{"cell_type":"code","source":"ypred = x @ theta.t() # matrix multiply; (N x 2) * (2 x 1) -> N x 1\nprint('ypred:\\n', ypred)\nloss = ((ypred-y)**2).mean() # mean squared error = MSE\nprint('mse loss: ', loss.item())\nloss.backward()\nprint('dL / d theta:\\n', theta.grad)\n# let's move W in that direction\ntheta.data.add_(-0.1 * theta.grad.data)\n# Now we will reset the gradient to zero.\ntheta.grad.zero_()\nprint('theta:\\n', theta)\nthetatrace.append(theta.data.clone()) # for logging","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.291853Z","iopub.execute_input":"2022-06-17T15:06:17.292343Z","iopub.status.idle":"2022-06-17T15:06:17.303713Z","shell.execute_reply.started":"2022-06-17T15:06:17.292291Z","shell.execute_reply":"2022-06-17T15:06:17.302724Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"You can re-execute this cell above a couple of times and see how $\\theta$ goes close towards the optimal value of `[3,0]`.","metadata":{}},{"cell_type":"code","source":"# Now let us plot in 2D what happened to theta during SGD optimization. In red is the true relation.\nthetas = torch.cat(thetatrace, dim=0).numpy()\nplt.figure()\nplt.plot(thetas[:,0], thetas[:, 1], 'x-')\nplt.plot(3, 0, 'ro')\nplt.xlabel('theta[0]')\nplt.ylabel('theta[1]')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.305628Z","iopub.execute_input":"2022-06-17T15:06:17.306081Z","iopub.status.idle":"2022-06-17T15:06:17.482741Z","shell.execute_reply.started":"2022-06-17T15:06:17.306041Z","shell.execute_reply":"2022-06-17T15:06:17.482018Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Ok, doing this manually gives you insight what happens down to the details. But usually we do not do the gradient updates manually, it would become very cumbersome if the net becomes more complex than the simple linear layer. pytorch gives us abstractions to easily manage this complexity: \n* `nn.Linear()` (or generally  `Module`s) which do two things: (a) they contain the learnable weight, and (b) define how they operate on an input tensor to give an output.\n* `module.zero_grad()` to clear the gradients, \n* `optim.SGD` with which you can do `optimizer.step()` to do a step of SGD.","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(23801)\nnet = nn.Linear(2,1, bias=False)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1) # do updates with `optimizer.step()`\n# x, y defined above. In a real problem we would typically get different x, y \"minibatches\"\n# of samples from a dataloader.\nfor i in range(100): # 10 optimization steps (gradient descent steps)\n    ypred = net(x)\n    loss = ((ypred-y)**2).mean() # mean squared error = MSE\n    optimizer.zero_grad()\n    loss.backward()\n    # and instead of W.data -= 0.1 * W.grad we do:\n    optimizer.step()\nprint(net.weight)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.484057Z","iopub.execute_input":"2022-06-17T15:06:17.484421Z","iopub.status.idle":"2022-06-17T15:06:17.516543Z","shell.execute_reply.started":"2022-06-17T15:06:17.484386Z","shell.execute_reply":"2022-06-17T15:06:17.515628Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### 4. Back to GANs: The min-max game.\nWe introduced and defined the generator G, the discriminator D, and the dataloader which will give us minibatches of real data. With the intermezzo on optimization we also understand how we optimize neural networks in pytorch.\n\nTo recap the basic idea of the min-max / adversarial game:\n* The Generator and Discriminator have competing objectives, they are \"adversaries\".\n* The Discriminator wants to assign high probability to real images and low probability to generated (fake) images\n* The Generator wants its generated images to look real, so wants to modify its outputs to get high scores from the Discriminator\n* We will optimize both alternatingly, with SGD steps (as before): optimize $\\theta_D$ the weights of $D(x, \\theta_D)$, and  $\\theta_G$ the weights of $G(z, \\theta_G)$.\n* Final goal of the whole min-max game is for the Generator to match the data distribution: $p_G(x) \\approx p_{data}(x)$.\n\n\nNow what are the objective functions for each of them? As mentioned in the introduction, the objective for the discriminator is to classify the real images as real, so $D(x) = 1$, and the fake images as fake, so $D(G(z))=0$.\nThis is a typical binary classification problem which calls for the binary cross-entropy (BCE) loss, which encourages exactly this solution.\n\nFor G we just try to minimize the same loss that D maximizes. See how G appears inside D? This shows how the output of the generator G is passed into the Discriminator to compute the loss.\n\n\nThis is the optimization problem:\n\n$$\n\\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]\n$$\n\nWe will do a single SGD step alternatingly to maximize D, then minimize G.\nIn fact for G we use a modified (non-saturing) loss $-\\log D(G(z))$. Different modifications of the loss and the relation to the distance between distributions $p_{data}$ and $p_{G}$ became a topic of research over the last years.\n","metadata":{}},{"cell_type":"code","source":"# Remember we have defined the discriminator and generator as:\nD = Discriminator()\nprint(D)\nG = Generator()\nprint(G)\n# Now let's set up the optimizers\noptimizerD = torch.optim.SGD(D.parameters(), lr=0.01)\noptimizerG = torch.optim.SGD(G.parameters(), lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.518025Z","iopub.execute_input":"2022-06-17T15:06:17.518522Z","iopub.status.idle":"2022-06-17T15:06:17.529762Z","shell.execute_reply.started":"2022-06-17T15:06:17.518482Z","shell.execute_reply":"2022-06-17T15:06:17.528497Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# and the BCE criterion which computes the loss above:\ncriterion = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.531227Z","iopub.execute_input":"2022-06-17T15:06:17.531655Z","iopub.status.idle":"2022-06-17T15:06:17.536478Z","shell.execute_reply.started":"2022-06-17T15:06:17.531615Z","shell.execute_reply":"2022-06-17T15:06:17.535375Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# STEP 1: Discriminator optimization step\nx_real, _ = iter(dataloader).next()\nlab_real = torch.ones(64, 1)\nlab_fake = torch.zeros(64, 1)\n# reset accumulated gradients from previous iteration\noptimizerD.zero_grad()\n\nD_x = D(x_real)\nlossD_real = criterion(D_x, lab_real)\n\nz = torch.randn(64, 100) # random noise, 64 samples, z_dim=100\nx_gen = G(z).detach()\nD_G_z = D(x_gen)\nlossD_fake = criterion(D_G_z, lab_fake)\n\nlossD = lossD_real + lossD_fake\nlossD.backward()\noptimizerD.step()\n\n# print(D_x.mean().item(), D_G_z.mean().item())","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.538490Z","iopub.execute_input":"2022-06-17T15:06:17.539336Z","iopub.status.idle":"2022-06-17T15:06:17.586210Z","shell.execute_reply.started":"2022-06-17T15:06:17.539284Z","shell.execute_reply":"2022-06-17T15:06:17.585510Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Some things to think about / try out / investigate:\n* what are the mean probabilities for real and fake? print them and see how they change when executing the cell above a couple of times. Does this correspond to your expectation?\n* can you confirm how the use of the criterion maps to the objective stated above?\n* when calling backward, the derivative of the loss wrt **what** gets computed?\n* what does `.detach()` do? Are the Generator parameters' gradients computed?","metadata":{}},{"cell_type":"code","source":"# STEP 2: Generator optimization step\n# note how only one of the terms involves the Generator so this is the only one that matters for G.\n# reset accumulated gradients from previous iteration\noptimizerG.zero_grad()\n\nz = torch.randn(64, 100) # random noise, 64 samples, z_dim=100\nD_G_z = D(G(z))\nlossG = criterion(D_G_z, lab_real) # -log D(G(z))\n\nlossG.backward()\noptimizerG.step()\n\nprint(D_G_z.mean().item())","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.587332Z","iopub.execute_input":"2022-06-17T15:06:17.587699Z","iopub.status.idle":"2022-06-17T15:06:17.603417Z","shell.execute_reply.started":"2022-06-17T15:06:17.587658Z","shell.execute_reply":"2022-06-17T15:06:17.602674Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Again run this cell a couple of times. See how the generator increases its Discriminator score?\n\nSome more things to ponder:\n* Do the Generator parameters now receive gradients? Why (compared to previous loop)?\n* From the definition of BCE loss confirm that this comes down to $-\\log D(G(z))$","metadata":{}},{"cell_type":"markdown","source":"### Putting it all together: the full training loop\n\nModifications to the code:\n* add device parameter to take GPU if available\n* use [Adam optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) (an adaptive learning-rate variation of SGD with momentum)\n* some very minimal logging","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint('Device: ', device)\n# Re-initialize D, G:\nD = Discriminator().to(device)\nG = Generator().to(device)\n# Now let's set up the optimizers (Adam, better than SGD for this)\noptimizerD = torch.optim.SGD(D.parameters(), lr=0.03)\noptimizerG = torch.optim.SGD(G.parameters(), lr=0.03)\n# optimizerD = torch.optim.Adam(D.parameters(), lr=0.0002)\n# optimizerG = torch.optim.Adam(G.parameters(), lr=0.0002)\nlab_real = torch.ones(64, 1, device=device)\nlab_fake = torch.zeros(64, 1, device=device)\n\n\n# for logging:\ncollect_x_gen = []\nfixed_noise = torch.randn(64, 100, device=device)\nfig = plt.figure() # keep updating this one\nplt.ion()\n\nfor epoch in range(10): # 3 epochs\n    for i, data in enumerate(dataloader, 0):\n        # STEP 1: Discriminator optimization step\n        x_real, _ = iter(dataloader).next()\n        x_real = x_real.to(device)\n        # reset accumulated gradients from previous iteration\n        optimizerD.zero_grad()\n\n        D_x = D(x_real)\n        lossD_real = criterion(D_x, lab_real)\n\n        z = torch.randn(64, 100, device=device) # random noise, 64 samples, z_dim=100\n        x_gen = G(z).detach()\n        D_G_z = D(x_gen)\n        lossD_fake = criterion(D_G_z, lab_fake)\n\n        lossD = lossD_real + lossD_fake\n        lossD.backward()\n        optimizerD.step()\n        \n        # STEP 2: Generator optimization step\n        # reset accumulated gradients from previous iteration\n        optimizerG.zero_grad()\n\n        z = torch.randn(64, 100, device=device) # random noise, 64 samples, z_dim=100\n        x_gen = G(z)\n        D_G_z = D(x_gen)\n        lossG = criterion(D_G_z, lab_real) # -log D(G(z))\n        \n        #print(lossG)\n\n        lossG.backward()\n        optimizerG.step()\n        if i % 100 == 0:\n            x_gen = G(fixed_noise)\n            show_imgs(x_gen, new_fig=False)\n            fig.canvas.draw()\n            print('e{}.i{}/{} last mb D(x)={:.4f} D(G(z))={:.4f}'.format(\n                epoch, i, len(dataloader), D_x.mean().item(), D_G_z.mean().item()))\n    # End of epoch\n    x_gen = G(fixed_noise)\n    collect_x_gen.append(x_gen.detach().clone())","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:06:17.606001Z","iopub.execute_input":"2022-06-17T15:06:17.606617Z","iopub.status.idle":"2022-06-17T15:11:12.419330Z","shell.execute_reply.started":"2022-06-17T15:06:17.606581Z","shell.execute_reply":"2022-06-17T15:11:12.418590Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"for x_gen in collect_x_gen:\n    show_imgs(x_gen)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:11:12.420694Z","iopub.execute_input":"2022-06-17T15:11:12.421262Z","iopub.status.idle":"2022-06-17T15:11:14.405270Z","shell.execute_reply.started":"2022-06-17T15:11:12.421216Z","shell.execute_reply":"2022-06-17T15:11:14.404473Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"f = torch.randn(8,100,device=device)\nx = G(f)\nshow_imgs(x)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T15:11:14.408711Z","iopub.execute_input":"2022-06-17T15:11:14.409111Z","iopub.status.idle":"2022-06-17T15:11:14.564561Z","shell.execute_reply.started":"2022-06-17T15:11:14.409070Z","shell.execute_reply":"2022-06-17T15:11:14.563789Z"},"trusted":true},"execution_count":26,"outputs":[]}]}